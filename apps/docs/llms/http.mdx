---
title: "Custom hosted LLMs"
description: "Using helf hosted models with SpinAI"
---

## Usage

```typescript
import { createAgent, createHttpLLM } from "spinai";

const llm = createHttpLLM({
  endpoint: "https://your-custom-llm-api.com/v1/chat/completions",
  apiKey: "your-api-key",
  headers: {
    "X-Custom-Header": "custom-value",
  },
  // Optional: Transform the request body
  transformRequest: (body) => ({
    prompt: body.messages[body.messages.length - 1].content,
    max_length: body.max_tokens,
    temperature: body.temperature,
  }),
  // Optional: Transform the response
  transformResponse: (response) => response.generated_text,
});

const agent = createAgent({
  instructions: "Help users with support tickets",
  actions: [getCustomerInfo, createTicket],
  llm,
});

const { response } = await agent({
  input: "I want a refund",
});
```

## Configuration

```typescript
interface HttpLLMConfig {
  endpoint: string;
  apiKey?: string;
  headers?: Record<string, string>;
  transformRequest?: (body: unknown) => unknown;
  transformResponse?: (response: unknown) => string;
}
```
